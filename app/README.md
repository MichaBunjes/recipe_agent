# Recipe Agent â€” LangGraph with Persistent Pantry

## Architecture

```
[START]
   â”‚
   â–¼
[load_pantry]  â† reads pantry.json from disk
   â”‚
   â–¼
[classify_intent]  â† LLM determines: recipe / add / remove / list
   â”‚
   â”œâ”€â”€ pantry_add â”€â”€â–º add to pantry.json â”€â”€â–º [END]
   â”œâ”€â”€ pantry_remove â”€â”€â–º remove from pantry.json â”€â”€â–º [END]
   â”œâ”€â”€ pantry_list â”€â”€â–º display grouped contents â”€â”€â–º [END]
   â”‚
   â–¼  (intent = "recipe")
[parse_input]  â† extract extras, constraints, prefs (pantry already loaded)
   â”‚
   â”œâ”€â”€ (needs_clarification?) â”€â”€â–º [END]
   â”‚
   â–¼
[generate_recipes]  â† 3 candidates using pantry + extras
   â”‚
   â–¼
   â¸ INTERRUPT â”€â”€ user picks recipe(s) or says "more"
   â”‚
   â–¼
[handle_selection]
   â”‚
   â”œâ”€â”€ (wants more + iteration < 3) â”€â”€â–º [generate_recipes]
   â”‚
   â–¼
[generate_grocery_list]  â† diffs pantry vs recipe needs
   â”‚
   â–¼
[format_output]  â† full instructions
   â”‚
   â–¼
[END]
```

## LangGraph patterns demonstrated

| Pattern | Where |
|---|---|
| **Intent-based routing** | `classify_intent` â†’ conditional edges to 4 branches |
| **Persistent external state** | `pantry.json` loaded/saved outside graph state |
| **Conditional edges** | `route_by_intent`, `route_after_parse`, `route_after_selection` |
| **Human-in-the-loop** | `interrupt_before=["handle_selection"]` |
| **Loop with cap** | `iteration_count < 3` on recipe regeneration |
| **Checkpointing** | `MemorySaver` for state across interrupts |

## File structure

```
recipe_agent/
â”œâ”€â”€ state.py          # TypedDict state schema
â”œâ”€â”€ pantry.py         # Pantry CRUD â€” reads/writes pantry.json
â”œâ”€â”€ nodes.py          # All node functions (LLM calls + pantry ops)
â”œâ”€â”€ graph.py          # Graph wiring, edges, interrupt config
â”œâ”€â”€ run.py            # Interactive REPL
â”œâ”€â”€ requirements.txt
â””â”€â”€ pantry.json       # Auto-created on first add (gitignore this)
```

## Setup

```bash
pip install -r requirements.txt
export OPENAI_API_KEY=sk-...
python run.py
```

## Example session

```
You> add chicken breast, rice, broccoli, soy sauce, garlic, eggs, onion, olive oil
ğŸ¤– Added to pantry: chicken breast, rice, broccoli, soy sauce, garlic, eggs, onion, olive oil. You now have 8 items.

You> show pantry
ğŸ¤– Your pantry:
   Protein: chicken breast (some)
   Vegetable: broccoli (some), garlic (some), onion (some)
   Grain: rice (some)
   Condiment: soy sauce (some), olive oil (some)
   Dairy: eggs (some)

You> quick asian dinner for 2
ğŸ¤– Using your pantry (8 items). Generating recipes...
ğŸ¤– Here are your options:
   1. Chicken Fried Rice â€” ...
   2. Garlic Chicken Stir-Fry â€” ...
   3. Egg Drop Soup with Rice â€” ...

You> 1
ğŸ¤– Going with: Chicken Fried Rice
ğŸ¤– You have everything you need â€” no shopping required.
ğŸ¤– ## Chicken Fried Rice ...
```

## Swapping LLMs

In `nodes.py`, change the `llm` line:

```python
# Anthropic
from langchain_anthropic import ChatAnthropic
llm = ChatAnthropic(model="claude-sonnet-4-20250514")

# Ollama (local, free)
from langchain_ollama import ChatOllama
llm = ChatOllama(model="llama3.1")
```

## Key design decision: pantry outside graph state

The pantry lives in a JSON file, not in LangGraph's checkpointer. Why:
- Pantry persists across all sessions (graph threads are per-conversation)
- Simple to manually edit, backup, or sync
- Graph state stays focused on the current request

If you later want multi-user support or more complex querying, swap `pantry.py` for SQLite â€” the interface stays the same.
